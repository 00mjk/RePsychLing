<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="Reinhold Kliegl" />

<meta name="date" content="2015-03-24" />

<title>RePsychLing Kronmüller and Barr (2007)</title>



<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; }
code > span.dt { color: #902000; }
code > span.dv { color: #40a070; }
code > span.bn { color: #40a070; }
code > span.fl { color: #40a070; }
code > span.ch { color: #4070a0; }
code > span.st { color: #4070a0; }
code > span.co { color: #60a0b0; font-style: italic; }
code > span.ot { color: #007020; }
code > span.al { color: #ff0000; font-weight: bold; }
code > span.fu { color: #06287e; }
code > span.er { color: #ff0000; font-weight: bold; }
</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>


<link href="data:text/css,body%20%7B%0A%20%20background%2Dcolor%3A%20%23fff%3B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20max%2Dwidth%3A%20700px%3B%0A%20%20overflow%3A%20visible%3B%0A%20%20padding%2Dleft%3A%202em%3B%0A%20%20padding%2Dright%3A%202em%3B%0A%20%20font%2Dfamily%3A%20%22Open%20Sans%22%2C%20%22Helvetica%20Neue%22%2C%20Helvetica%2C%20Arial%2C%20sans%2Dserif%3B%0A%20%20font%2Dsize%3A%2014px%3B%0A%20%20line%2Dheight%3A%201%2E35%3B%0A%7D%0A%0A%23header%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0A%0A%23TOC%20%7B%0A%20%20clear%3A%20both%3B%0A%20%20margin%3A%200%200%2010px%2010px%3B%0A%20%20padding%3A%204px%3B%0A%20%20width%3A%20400px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20border%2Dradius%3A%205px%3B%0A%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20font%2Dsize%3A%2013px%3B%0A%20%20line%2Dheight%3A%201%2E3%3B%0A%7D%0A%20%20%23TOC%20%2Etoctitle%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%20%20font%2Dsize%3A%2015px%3B%0A%20%20%20%20margin%2Dleft%3A%205px%3B%0A%20%20%7D%0A%0A%20%20%23TOC%20ul%20%7B%0A%20%20%20%20padding%2Dleft%3A%2040px%3B%0A%20%20%20%20margin%2Dleft%3A%20%2D1%2E5em%3B%0A%20%20%20%20margin%2Dtop%3A%205px%3B%0A%20%20%20%20margin%2Dbottom%3A%205px%3B%0A%20%20%7D%0A%20%20%23TOC%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dleft%3A%20%2D2em%3B%0A%20%20%7D%0A%20%20%23TOC%20li%20%7B%0A%20%20%20%20line%2Dheight%3A%2016px%3B%0A%20%20%7D%0A%0Atable%20%7B%0A%20%20margin%3A%201em%20auto%3B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dcolor%3A%20%23DDDDDD%3B%0A%20%20border%2Dstyle%3A%20outset%3B%0A%20%20border%2Dcollapse%3A%20collapse%3B%0A%7D%0Atable%20th%20%7B%0A%20%20border%2Dwidth%3A%202px%3B%0A%20%20padding%3A%205px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%7D%0Atable%20td%20%7B%0A%20%20border%2Dwidth%3A%201px%3B%0A%20%20border%2Dstyle%3A%20inset%3B%0A%20%20line%2Dheight%3A%2018px%3B%0A%20%20padding%3A%205px%205px%3B%0A%7D%0Atable%2C%20table%20th%2C%20table%20td%20%7B%0A%20%20border%2Dleft%2Dstyle%3A%20none%3B%0A%20%20border%2Dright%2Dstyle%3A%20none%3B%0A%7D%0Atable%20thead%2C%20table%20tr%2Eeven%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Ap%20%7B%0A%20%20margin%3A%200%2E5em%200%3B%0A%7D%0A%0Ablockquote%20%7B%0A%20%20background%2Dcolor%3A%20%23f6f6f6%3B%0A%20%20padding%3A%200%2E25em%200%2E75em%3B%0A%7D%0A%0Ahr%20%7B%0A%20%20border%2Dstyle%3A%20solid%3B%0A%20%20border%3A%20none%3B%0A%20%20border%2Dtop%3A%201px%20solid%20%23777%3B%0A%20%20margin%3A%2028px%200%3B%0A%7D%0A%0Adl%20%7B%0A%20%20margin%2Dleft%3A%200%3B%0A%7D%0A%20%20dl%20dd%20%7B%0A%20%20%20%20margin%2Dbottom%3A%2013px%3B%0A%20%20%20%20margin%2Dleft%3A%2013px%3B%0A%20%20%7D%0A%20%20dl%20dt%20%7B%0A%20%20%20%20font%2Dweight%3A%20bold%3B%0A%20%20%7D%0A%0Aul%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%7D%0A%20%20ul%20li%20%7B%0A%20%20%20%20list%2Dstyle%3A%20circle%20outside%3B%0A%20%20%7D%0A%20%20ul%20ul%20%7B%0A%20%20%20%20margin%2Dbottom%3A%200%3B%0A%20%20%7D%0A%0Apre%2C%20code%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20color%3A%20%23333%3B%0A%7D%0Apre%20%7B%0A%20%20white%2Dspace%3A%20pre%2Dwrap%3B%20%20%20%20%2F%2A%20Wrap%20long%20lines%20%2A%2F%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20margin%3A%205px%200px%2010px%200px%3B%0A%20%20padding%3A%2010px%3B%0A%7D%0Apre%3Anot%28%5Bclass%5D%29%20%7B%0A%20%20background%2Dcolor%3A%20%23f7f7f7%3B%0A%7D%0A%0Acode%20%7B%0A%20%20font%2Dfamily%3A%20Consolas%2C%20Monaco%2C%20%27Courier%20New%27%2C%20monospace%3B%0A%20%20font%2Dsize%3A%2085%25%3B%0A%7D%0Ap%20%3E%20code%2C%20li%20%3E%20code%20%7B%0A%20%20padding%3A%202px%200px%3B%0A%7D%0A%0Adiv%2Efigure%20%7B%0A%20%20text%2Dalign%3A%20center%3B%0A%7D%0Aimg%20%7B%0A%20%20background%2Dcolor%3A%20%23FFFFFF%3B%0A%20%20padding%3A%202px%3B%0A%20%20border%3A%201px%20solid%20%23DDDDDD%3B%0A%20%20border%2Dradius%3A%203px%3B%0A%20%20border%3A%201px%20solid%20%23CCCCCC%3B%0A%20%20margin%3A%200%205px%3B%0A%7D%0A%0Ah1%20%7B%0A%20%20margin%2Dtop%3A%200%3B%0A%20%20font%2Dsize%3A%2035px%3B%0A%20%20line%2Dheight%3A%2040px%3B%0A%7D%0A%0Ah2%20%7B%0A%20%20border%2Dbottom%3A%204px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20padding%2Dbottom%3A%202px%3B%0A%20%20font%2Dsize%3A%20145%25%3B%0A%7D%0A%0Ah3%20%7B%0A%20%20border%2Dbottom%3A%202px%20solid%20%23f7f7f7%3B%0A%20%20padding%2Dtop%3A%2010px%3B%0A%20%20font%2Dsize%3A%20120%25%3B%0A%7D%0A%0Ah4%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23f7f7f7%3B%0A%20%20margin%2Dleft%3A%208px%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Ah5%2C%20h6%20%7B%0A%20%20border%2Dbottom%3A%201px%20solid%20%23ccc%3B%0A%20%20font%2Dsize%3A%20105%25%3B%0A%7D%0A%0Aa%20%7B%0A%20%20color%3A%20%230033dd%3B%0A%20%20text%2Ddecoration%3A%20none%3B%0A%7D%0A%20%20a%3Ahover%20%7B%0A%20%20%20%20color%3A%20%236666ff%3B%20%7D%0A%20%20a%3Avisited%20%7B%0A%20%20%20%20color%3A%20%23800080%3B%20%7D%0A%20%20a%3Avisited%3Ahover%20%7B%0A%20%20%20%20color%3A%20%23BB00BB%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22http%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%20%20a%5Bhref%5E%3D%22https%3A%22%5D%20%7B%0A%20%20%20%20text%2Ddecoration%3A%20underline%3B%20%7D%0A%0A%2F%2A%20Class%20described%20in%20https%3A%2F%2Fbenjeffrey%2Ecom%2Fposts%2Fpandoc%2Dsyntax%2Dhighlighting%2Dcss%0A%20%20%20Colours%20from%20https%3A%2F%2Fgist%2Egithub%2Ecom%2Frobsimmons%2F1172277%20%2A%2F%0A%0Acode%20%3E%20span%2Ekw%20%7B%20color%3A%20%23555%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Keyword%20%2A%2F%0Acode%20%3E%20span%2Edt%20%7B%20color%3A%20%23902000%3B%20%7D%20%2F%2A%20DataType%20%2A%2F%0Acode%20%3E%20span%2Edv%20%7B%20color%3A%20%2340a070%3B%20%7D%20%2F%2A%20DecVal%20%28decimal%20values%29%20%2A%2F%0Acode%20%3E%20span%2Ebn%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20BaseN%20%2A%2F%0Acode%20%3E%20span%2Efl%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Float%20%2A%2F%0Acode%20%3E%20span%2Ech%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20Char%20%2A%2F%0Acode%20%3E%20span%2Est%20%7B%20color%3A%20%23d14%3B%20%7D%20%2F%2A%20String%20%2A%2F%0Acode%20%3E%20span%2Eco%20%7B%20color%3A%20%23888888%3B%20font%2Dstyle%3A%20italic%3B%20%7D%20%2F%2A%20Comment%20%2A%2F%0Acode%20%3E%20span%2Eot%20%7B%20color%3A%20%23007020%3B%20%7D%20%2F%2A%20OtherToken%20%2A%2F%0Acode%20%3E%20span%2Eal%20%7B%20color%3A%20%23ff0000%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20AlertToken%20%2A%2F%0Acode%20%3E%20span%2Efu%20%7B%20color%3A%20%23900%3B%20font%2Dweight%3A%20bold%3B%20%7D%20%2F%2A%20Function%20calls%20%2A%2F%20%0Acode%20%3E%20span%2Eer%20%7B%20color%3A%20%23a61717%3B%20background%2Dcolor%3A%20%23e3d2d2%3B%20%7D%20%2F%2A%20ErrorTok%20%2A%2F%0A%0A" rel="stylesheet" type="text/css" />

</head>

<body>



<div id="header">
<h1 class="title">RePsychLing Kronmüller and Barr (2007)</h1>
<h4 class="author"><em>Reinhold Kliegl</em></h4>
<h4 class="date"><em>2015-03-24</em></h4>
</div>


<p>We apply the iterative reduction of LMM complexity to truncated response times of a 2x2x2 factorial psycholinguistic experiment (<span class="citation">Kronm<span>ü</span>ller and Barr (2007)</span>, Exp. 2; reanalyzed with an LMM in <span class="citation">Barr et al. (2013)</span>). The data are from 56 subjects who responded to 32 items. Specifically, subjects had to select one of several objects presented on a monitor with a cursor. The manipulations involved (1) auditory instructions that maintained or broke a precedent of reference for the objects established over prior trials, (2) with the instruction being presented by the speaker who established the precedent (i.e., an old speaker) or a new speaker, and (3) whether the task had to be performed without or with a cognitive load consisting of six random digits. All factors were varied within subjects and within items. There were main effects of Load, Speaker, and Precedent; none of the interactions were significant. Although standard errors of fixed-effect coefficents varied slightly across models, our reanalyses afforded the same statistical inference about the experimental manipulations as the original article, irrespective of LMM specification. The purpose of the analysis is to illustrate an assessment of model complexity as far as variance components and correlation parameters are concerned, neither of which were in the focus of the original publication.</p>
<div id="data-from-kronmullerbarr2007" class="section level2">
<h2>Data from <span class="citation">Kronm<span>ü</span>ller and Barr (2007)</span></h2>
<p>The data are available as <code>kb07</code> in the <code>RePsychLing</code> package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(kb07)</code></pre>
<pre><code>'data.frame':   1790 obs. of  10 variables:
 $ subj   : Factor w/ 56 levels &quot;30&quot;,&quot;31&quot;,&quot;34&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...
 $ item   : Factor w/ 32 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ...
 $ RTtrunc: num  2267 3856 1567 1732 2660 ...
 $ S      : num  1 -1 -1 1 1 -1 -1 1 1 -1 ...
 $ P      : num  -1 1 -1 1 -1 1 -1 1 -1 1 ...
 $ C      : num  1 -1 -1 -1 -1 1 1 1 1 -1 ...
 $ SP     : num  -1 -1 1 1 -1 -1 1 1 -1 -1 ...
 $ SC     : num  1 1 1 -1 -1 -1 -1 1 1 1 ...
 $ PC     : num  -1 -1 1 -1 1 1 -1 1 -1 -1 ...
 $ SPC    : num  -1 1 -1 -1 1 -1 1 1 -1 1 ...</code></pre>
<div id="maximal-linear-mixed-model-maxlmm" class="section level3">
<h3>Maximal linear mixed model (<em>maxLMM</em>)</h3>
<p>Barr et al. (2012, supplement) analyzed Kronmüller and Barr (2007, Exp. 2) with the <em>maxLMM</em> comprising 16 variance components (eight each for the random factors <code>subj</code> and <code>item</code>, respectively) (Footnote below output). This model takes a long time to fit using <code>lmer</code> because there are so many parameters and the likelihood surface is very flat. The <code>lmm</code> function from the <a href="https://github.com/dmbates/MixedModels.jl">MixedModels</a> package for <a href="http://julialang.org">Julia</a> is much faster fitting this particular model, providing the results</p>
<pre class="sourceCode r"><code class="sourceCode r">m0 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>+S+P+C+SP+SC+PC+SPC|subj) +
<span class="st">           </span>(<span class="dv">1</span>+S+P+C+SP+SC+PC+SPC|item), kb07, <span class="dt">REML=</span><span class="ot">FALSE</span>, <span class="dt">start=</span>thcvg$kb07$m0,
           <span class="dt">control=</span><span class="kw">lmerControl</span>(<span class="dt">optimizer=</span><span class="st">&quot;Nelder_Mead&quot;</span>,<span class="dt">optCtrl=</span><span class="kw">list</span>(<span class="dt">maxfun=</span>1L),
                              <span class="dt">check.conv.grad=</span><span class="st">&quot;ignore&quot;</span>,<span class="dt">check.conv.hess=</span><span class="st">&quot;ignore&quot;</span>))</code></pre>
<pre><code>Warning in (function (fn, par, lower = rep.int(-Inf, n), upper = rep.int(Inf, : failure to
converge in 1 evaluations</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">summary</span>(m0),<span class="dt">corr=</span><span class="ot">FALSE</span>)</code></pre>
<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 + S + P + C +  
    SP + SC + PC + SPC | subj) + (1 + S + P + C + SP + SC + PC +      SPC | item)
   Data: kb07
Control: lmerControl(optimizer = &quot;Nelder_Mead&quot;, optCtrl = list(maxfun = 1L),  
    check.conv.grad = &quot;ignore&quot;, check.conv.hess = &quot;ignore&quot;)

     AIC      BIC   logLik deviance df.resid 
 28748.3  29193.0 -14293.2  28586.3     1709 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.0333 -0.5774 -0.1438  0.3964  4.7091 

Random effects:
 Groups   Name        Variance Std.Dev. Corr                                     
 subj     (Intercept)  90769   301.28                                            
          S             5182    71.99   -0.43                                    
          P             5544    74.46   -0.47  0.07                              
          C             7587    87.10    0.21 -0.20  0.41                        
          SP            8830    93.97    0.20 -0.76 -0.54 -0.20                  
          SC            1821    42.68    0.47 -0.53 -0.11 -0.44  0.28            
          PC            7423    86.15   -0.10  0.13 -0.05 -0.86 -0.06  0.70      
          SPC           3802    61.66   -0.48  0.41 -0.39 -0.09  0.18 -0.78 -0.39
 item     (Intercept) 129825   360.31                                            
          S             1855    43.07   -0.34                                    
          P            62393   249.79   -0.68 -0.45                              
          C             2949    54.30    0.20 -0.03 -0.18                        
          SP            1043    32.29    0.57 -0.76  0.02  0.01                  
          SC            1620    40.25    0.28 -0.03 -0.27  0.44 -0.21            
          PC            4700    68.56    0.08 -0.23  0.21 -0.13 -0.26  0.02      
          SPC           4820    69.43    0.04 -0.48  0.32 -0.68  0.65 -0.68 -0.10
 Residual             399612   632.15                                            
Number of obs: 1790, groups:  subj, 56; item, 32

Fixed effects:
            Estimate Std. Error t value
(Intercept) 2180.627     76.819  28.387
S            -66.990     19.334  -3.465
P           -333.881     47.666  -7.005
C             78.987     21.235   3.720
SP            22.152     20.336   1.089
SC           -18.924     17.505  -1.081
PC             5.262     22.421   0.235
SPC          -23.951     21.019  -1.139
convergence code: 0
failure to converge in 1 evaluations</code></pre>
<p>This fit converges and produces what look like reasonable parameter estimates (i.e., no variance components with estimates close to zero; no correlation parameters with values close to <span class="math">\(\pm1\)</span>).</p>
<p>We started this model fit at the converged parameter estimates to save time. Starting from the usual initial values, the model fit took nearly 40,000 iterations for the nonlinear optimizer to converge. Theparameter values look reasonable but are a local optimum. We use a better parameter value here.</p>
<p>The slow convergence is due to a total of 2 x 36 = 72 parameters in the optimization. These parameters are all in the relative covariance factors. The more easily estimated nine fixed-effects parameters have been “profiled out” of the optimization.</p>
<p>Footnote: The model formula reported in the supplement of Barr et al. (2012) specified only five variance components for the random factor item. However, <code>lmer()</code> automatically includes all lower-order terms of interactions specified for random-factor terms, resulting in the <em>maxLMM</em> for this experimental design.</p>
</div>
<div id="evaluation-of-singular-value-decomposition-svd-for-maxlmm" class="section level3">
<h3>Evaluation of singular value decomposition (svd) for <em>maxLMM</em></h3>
<p>Considering that there are only 56 subjects and 32 items it is quite optimistic to expect to estimate 36 highly nonlinear covariance parameters for <code>subj</code> and another 36 for <code>item</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">rePCA</span>(m0))</code></pre>
<pre><code>$subj
Importance of components:
                         [,1]   [,2]    [,3]    [,4] [,5] [,6] [,7] [,8]
Standard deviation     0.4881 0.1989 0.17474 0.13901    0    0    0    0
Proportion of Variance 0.7271 0.1207 0.09317 0.05897    0    0    0    0
Cumulative Proportion  0.7271 0.8479 0.94103 1.00000    1    1    1    1

$item
Importance of components:
                         [,1]   [,2]    [,3]    [,4]    [,5] [,6] [,7] [,8]
Standard deviation     0.6449 0.2772 0.13070 0.10481 0.05281    0    0    0
Proportion of Variance 0.7943 0.1467 0.03263 0.02098 0.00533    0    0    0
Cumulative Proportion  0.7943 0.9411 0.97369 0.99467 1.00000    1    1    1</code></pre>
<p>The directions are the principal components for this covariance matrix. We see that there are seven singular values of zero, that is there is zero variability in seven directions. Overall, the svd analysis of this model returns only eight principal components with variances larger than one percent. Thus, the <em>maxLMM</em> is clearly too complex.</p>
</div>
<div id="zero-correlation-parameter-linear-mixed-model-zcplmm" class="section level3">
<h3>Zero-correlation-parameter linear mixed model (<em>zcpLMM</em>)</h3>
<p>As a first step of model reduction, we propose to start with a model including all 16 variance components, but no correlation parameters. Note that here we go through the motion to be consistent with the recommended strategy. The large number of components with zero or close to zero variance in <em>maxLMM</em> already strongly suggests the need for a reduction of the number of variance components–as done in the next step. For this <em>zcpLMM</em>, we extract the vector-valued variables from the model matrix without the intercept column which is provided by the R formula. Then, we use the new double-bar syntax for <code>lmer()</code> to force correlation parameters to zero.</p>
<pre class="sourceCode r"><code class="sourceCode r">m1 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>+S+P+C+SP+SC+PC+SPC||subj) +
<span class="st">             </span>(<span class="dv">1</span>+S+P+C+SP+SC+PC+SPC||item), kb07, <span class="dt">REML=</span><span class="ot">FALSE</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(m1),<span class="dt">corr=</span><span class="ot">FALSE</span>)</code></pre>
<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) +  
    (0 + S | subj) + (0 + P | subj) + (0 + C | subj) + (0 + SP |  
    subj) + (0 + SC | subj) + (0 + PC | subj) + (0 + SPC | subj)) +  
    ((1 | item) + (0 + S | item) + (0 + P | item) + (0 + C |  
        item) + (0 + SP | item) + (0 + SC | item) + (0 + PC |  
        item) + (0 + SPC | item))
   Data: kb07

     AIC      BIC   logLik deviance df.resid 
 28720.9  28858.2 -14335.5  28670.9     1765 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.7153 -0.5874 -0.1496  0.3696  4.3417 

Random effects:
 Groups   Name        Variance  Std.Dev. 
 subj     (Intercept) 9.174e+04 3.029e+02
 subj.1   S           9.820e+02 3.134e+01
 subj.2   P           2.115e+03 4.599e+01
 subj.3   C           3.886e+03 6.233e+01
 subj.4   SP          5.566e+03 7.461e+01
 subj.5   SC          0.000e+00 0.000e+00
 subj.6   PC          4.209e+03 6.488e+01
 subj.7   SPC         0.000e+00 0.000e+00
 item     (Intercept) 1.323e+05 3.637e+02
 item.1   S           1.072e-10 1.036e-05
 item.2   P           6.238e+04 2.498e+02
 item.3   C           2.802e+03 5.293e+01
 item.4   SP          1.001e-10 1.001e-05
 item.5   SC          7.364e+02 2.714e+01
 item.6   PC          3.500e+03 5.916e+01
 item.7   SPC         2.459e+03 4.959e+01
 Residual             4.315e+05 6.569e+02
Number of obs: 1790, groups:  subj, 56; item, 32

Fixed effects:
            Estimate Std. Error t value
(Intercept) 2180.613     77.549  28.119
S            -67.041     16.082  -4.169
P           -333.830     47.204  -7.072
C             79.002     19.951   3.960
SP            22.166     18.452   1.201
SC           -18.873     16.251  -1.161
PC             5.211     20.631   0.253
SPC          -23.966     17.830  -1.344</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m1, m0)  </code></pre>
<pre><code>Data: kb07
Models:
m1: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m1:     (0 + S | subj) + (0 + P | subj) + (0 + C | subj) + (0 + SP | 
m1:     subj) + (0 + SC | subj) + (0 + PC | subj) + (0 + SPC | subj)) + 
m1:     ((1 | item) + (0 + S | item) + (0 + P | item) + (0 + C | 
m1:         item) + (0 + SP | item) + (0 + SC | item) + (0 + PC | 
m1:         item) + (0 + SPC | item))
m0: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 + S + P + C + 
m0:     SP + SC + PC + SPC | subj) + (1 + S + P + C + SP + SC + PC + 
m0:     SPC | item)
   Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m1 25 28721 28858 -14336    28671                         
m0 81 28748 29193 -14293    28586 84.596     56   0.008094</code></pre>
<p>Nominally, the <em>zcpLMM</em> fits significantly worse than the <em>maxLMM</em>, but note that the ^2 for the LRT (85) is smaller than twice the degrees of freedom for the LRT (56). Also the degrees of freedom are somewhat of an underestimate. According to our judgement, <em>zcpLMM</em> could be preferred to <em>maxLMM</em>.</p>
</div>
<div id="principal-components-analysis-for-zcplmm" class="section level3">
<h3>Principal components analysis for <em>zcpLMM</em></h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">rePCA</span>(m1))</code></pre>
<pre><code>$subj
Importance of components:
                         [,1]   [,2]    [,3]    [,4]    [,5]    [,6] [,7] [,8]
Standard deviation     0.4611 0.1136 0.09877 0.09489 0.07001 0.04771    0    0
Proportion of Variance 0.8455 0.0513 0.03880 0.03581 0.01949 0.00905    0    0
Cumulative Proportion  0.8455 0.8969 0.93565 0.97146 0.99095 1.00000    1    1

$item
Importance of components:
                         [,1]   [,2]    [,3]    [,4]    [,5]    [,6]      [,7]      [,8]
Standard deviation     0.5537 0.3802 0.09006 0.08058 0.07549 0.04131 1.576e-08 1.523e-08
Proportion of Variance 0.6480 0.3055 0.01714 0.01372 0.01204 0.00361 0.000e+00 0.000e+00
Cumulative Proportion  0.6480 0.9535 0.97063 0.98435 0.99639 1.00000 1.000e+00 1.000e+00</code></pre>
<p>The PCM analysis of <em>zcpLMM</em> returns 12 of 16 components with variances different from zero. Thus, using this result as guide, the <em>zcpLMM</em> is still too complex. Inspection of <em>zcpLMM</em> variance components (see <em>zcpLMM</em> <code>m1</code>) suggests a further reduction of model complexity with drop1-LRT tests, starting with the smallest variance components.</p>
</div>
<div id="dropping-non-significant-variance-components" class="section level3">
<h3>Dropping non-significant variance components</h3>
<p>A second step of model reduction is to remove variance components that are not significant according to a likelihood ratio test (LRT). Starting with the smallest variance component (or a set of them) this step can be repeated until significant change in goodness of fit is indicated. For the present case, variance components for <code>SC</code> and <code>SPC</code> for <code>subj</code> and <code>S</code> and <code>SP</code> for <code>item</code> are estimated with zero values. We refit the LMM without these variance components.</p>
<pre class="sourceCode r"><code class="sourceCode r">m2 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>+S+P+C+SP+PC||subj) +
<span class="st">             </span>(<span class="dv">1</span>+P+C+SC+PC+SPC||item), kb07, <span class="dt">REML=</span><span class="ot">FALSE</span>)
<span class="kw">anova</span>(m2, m1)  <span class="co"># not significant: prefer m2 over m1</span></code></pre>
<pre><code>Data: kb07
Models:
m2: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m2:     (0 + S | subj) + (0 + P | subj) + (0 + C | subj) + (0 + SP | 
m2:     subj) + (0 + PC | subj)) + ((1 | item) + (0 + P | item) + 
m2:     (0 + C | item) + (0 + SC | item) + (0 + PC | item) + (0 + 
m2:     SPC | item))
m1: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m1:     (0 + S | subj) + (0 + P | subj) + (0 + C | subj) + (0 + SP | 
m1:     subj) + (0 + SC | subj) + (0 + PC | subj) + (0 + SPC | subj)) + 
m1:     ((1 | item) + (0 + S | item) + (0 + P | item) + (0 + C | 
m1:         item) + (0 + SP | item) + (0 + SC | item) + (0 + PC | 
m1:         item) + (0 + SPC | item))
   Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
m2 21 28713 28828 -14336    28671                        
m1 25 28721 28858 -14336    28671     0      4          1</code></pre>
<p>Obviously, these four variance components are not supported by information in the data. So we drop the next four smallest variance components, vc1 and vc2 for <code>subj</code> and vc5 and vc7 for <code>item</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">m3 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>+C+SP+PC||subj) +<span class="st">  </span>(<span class="dv">1</span>+P+C+PC||item),kb07,<span class="dt">REML=</span><span class="ot">FALSE</span>)
<span class="kw">anova</span>(m3,  m2)  <span class="co"># not significant: prefer m3 over m2</span></code></pre>
<pre><code>Data: kb07
Models:
m3: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m3:     (0 + C | subj) + (0 + SP | subj) + (0 + PC | subj)) + ((1 | 
m3:     item) + (0 + P | item) + (0 + C | item) + (0 + PC | item))
m2: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m2:     (0 + S | subj) + (0 + P | subj) + (0 + C | subj) + (0 + SP | 
m2:     subj) + (0 + PC | subj)) + ((1 | item) + (0 + P | item) + 
m2:     (0 + C | item) + (0 + SC | item) + (0 + PC | item) + (0 + 
m2:     SPC | item))
   Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m3 17 28707 28800 -14336    28673                         
m2 21 28713 28828 -14336    28671 2.0111      4     0.7337</code></pre>
<p>There is no significant drop in goodness of fit. Therefore, we continue with dropping vc3, vc4, and vc6 for <code>subj</code> and vc3 and vc6 for <code>item</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">m4 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>|subj) +<span class="st"> </span>(<span class="dv">1</span>|item) +<span class="st"> </span>(<span class="dv">0</span>+P|item),kb07,<span class="dt">REML=</span><span class="ot">FALSE</span>)
<span class="kw">anova</span>(m4, m3)  <span class="co"># not significant: prefer m4 over m3</span></code></pre>
<pre><code>Data: kb07
Models:
m4: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 | 
m4:     item) + (0 + P | item)
m3: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m3:     (0 + C | subj) + (0 + SP | subj) + (0 + PC | subj)) + ((1 | 
m3:     item) + (0 + P | item) + (0 + C | item) + (0 + PC | item))
   Df   AIC   BIC logLik deviance Chisq Chi Df Pr(&gt;Chisq)
m4 12 28706 28772 -14341    28682                        
m3 17 28707 28800 -14336    28673 9.143      5     0.1035</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m4, m1)  <span class="co"># not significant: prefer m4 over m1 (no accumulation)</span></code></pre>
<pre><code>Data: kb07
Models:
m4: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 | 
m4:     item) + (0 + P | item)
m1: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + ((1 | subj) + 
m1:     (0 + S | subj) + (0 + P | subj) + (0 + C | subj) + (0 + SP | 
m1:     subj) + (0 + SC | subj) + (0 + PC | subj) + (0 + SPC | subj)) + 
m1:     ((1 | item) + (0 + S | item) + (0 + P | item) + (0 + C | 
m1:         item) + (0 + SP | item) + (0 + SC | item) + (0 + PC | 
m1:         item) + (0 + SPC | item))
   Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m4 12 28706 28772 -14341    28682                         
m1 25 28721 28858 -14336    28671 11.154     13     0.5979</code></pre>
<p>As a final test, we refit the LMM without vc2 for <code>item</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">m5 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>|subj) +<span class="st"> </span>(<span class="dv">1</span>|item), <span class="dt">data=</span>kb07, <span class="dt">REML=</span><span class="ot">FALSE</span>)
<span class="kw">anova</span>(m5, m4)  <span class="co"># significant: prefer m4 over m5</span></code></pre>
<pre><code>Data: kb07
Models:
m5: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 | 
m5:     item)
m4: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 | 
m4:     item) + (0 + P | item)
   Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m5 11 28848 28909 -14413    28826                         
m4 12 28706 28772 -14341    28682 144.37      1  &lt; 2.2e-16</code></pre>
<p>This time the LRT is significant. Therefore, we stay with LMM <code>m4</code> and test correlation parameters for this model.</p>
</div>
<div id="extending-the-reduced-lmm-with-a-correlation-parameter" class="section level3">
<h3>Extending the reduced LMM with a correlation parameter</h3>
<pre class="sourceCode r"><code class="sourceCode r">m6 &lt;-<span class="st"> </span><span class="kw">lmer</span>(RTtrunc ~<span class="st"> </span><span class="dv">1</span>+S+P+C+SP+SC+PC+SPC +<span class="st"> </span>(<span class="dv">1</span>|subj) +<span class="st"> </span>(<span class="dv">1</span>+P|item), kb07, <span class="dt">REML=</span><span class="ot">FALSE</span>)
<span class="kw">print</span>(<span class="kw">summary</span>(m6), <span class="dt">corr=</span><span class="ot">FALSE</span>)</code></pre>
<pre><code>Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 +      P | item)
   Data: kb07

     AIC      BIC   logLik deviance df.resid 
 28691.7  28763.1 -14332.9  28665.7     1777 

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-2.6564 -0.5887 -0.1573  0.4161  4.3966 

Random effects:
 Groups   Name        Variance Std.Dev. Corr 
 subj     (Intercept)  89104   298.5         
 item     (Intercept) 132399   363.9         
          P            63832   252.7    -0.69
 Residual             458506   677.1         
Number of obs: 1790, groups:  subj, 56; item, 32

Fixed effects:
            Estimate Std. Error t value
(Intercept) 2180.567     77.361  28.187
S            -67.107     16.005  -4.193
P           -333.764     47.444  -7.035
C             79.047     16.005   4.939
SP            22.212     16.005   1.388
SC           -18.807     16.005  -1.175
PC             5.145     16.005   0.321
SPC          -24.011     16.005  -1.500</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m4, m6)  <span class="co"># significant: prefer m6 over m4</span></code></pre>
<pre><code>Data: kb07
Models:
m4: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 | 
m4:     item) + (0 + P | item)
m6: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 + 
m6:     P | item)
   Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m4 12 28706 28772 -14341    28682                         
m6 13 28692 28763 -14333    28666 16.335      1  5.307e-05</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">anova</span>(m6, m0)  <span class="co"># not significant: prefer m6 over m0 (no accumulation)</span></code></pre>
<pre><code>Data: kb07
Models:
m6: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 | subj) + (1 + 
m6:     P | item)
m0: RTtrunc ~ 1 + S + P + C + SP + SC + PC + SPC + (1 + S + P + C + 
m0:     SP + SC + PC + SPC | subj) + (1 + S + P + C + SP + SC + PC + 
m0:     SPC | item)
   Df   AIC   BIC logLik deviance  Chisq Chi Df Pr(&gt;Chisq)
m6 13 28692 28763 -14333    28666                         
m0 81 28748 29193 -14293    28586 79.415     68     0.1622</code></pre>
<p>There is evidence for a reliable item-related negative correlation parameter between mean and precedence effect, that is there are reliable differences between items in the precedence effect. Finally, there is no significant difference between LMM <code>m6</code> and the <em>maxLMM</em> <code>m0</code>. The final number of reliable dimensions is actually smaller than suggested by the PCA analysis of the <em>maxLMM</em> <code>m0</code>.</p>
</div>
<div id="profiling-the-parameters" class="section level3">
<h3>Profiling the parameters</h3>
<p>Confidence intervals for all parameters can be obtained</p>
</div>
<div id="summary" class="section level3">
<h3>Summary</h3>
<p>In our opinion, <code>m6</code> is the <em>optimal</em> LMM for the data of this experiment. The general strategy of (1) starting with <em>maxLMM</em>, (2) followed by <em>zcpLMM</em>, (3) followed by iteratively dropping variance components until there is a significant decrease in goodness of model fit, (4) followed by inclusion of correlation parameters for the remaining variance components, and (5) using svd all along the way to check the principal dimensionality of the data for the respective intermediate models worked quite well again. Indeed, we also reanalyzed two additional experiments reported in the supplement of Barr et al. (2012). As documented in the <code>RePsychLing</code> package accompanying the present article, in each case, the <em>maxLMM</em> was too complex for the information provided by the experimental data. In each case, the data supported only a very sparse random-effects structure beyond varying intercepts for subjects and items. Fortunately and interestingly, none of the analyses changed the statistical inference about fixed effects in these experiments. Obviously, this cannot be ruled out in general. If authors adhere to a strict criterion for significance, such as p &lt; .05 suitably adjusted for multiple comparisons, there is always a chance that a t-value will fall above or below the criterion across different versions of an LMM.</p>
<p>Given the degree of deconstruction (i.e., model simplification) reported for these models, one may wonder whether it might be more efficient to iteratively <em>increase</em> rather the <em>decrease</em> LMM complexity, that is to start with a minimal linear mixed model (<em>minLMM</em>), varying only intercepts of subject and item factors and adding variance components and correlation parameters to such a model. We will turn to this strategy in the next section.</p>
</div>
</div>
<div id="versions-of-packages-used" class="section level2">
<h2>Versions of packages used</h2>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sessionInfo</span>()</code></pre>
<pre><code>R version 3.1.3 (2015-03-09)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.10

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C               LC_TIME=en_US.UTF-8       
 [4] LC_COLLATE=en_US.UTF-8     LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                  LC_ADDRESS=C              
[10] LC_TELEPHONE=C             LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] knitr_1.9.5       RePsychLing_0.0.3 lme4_1.1-8        Rcpp_0.11.5      
[5] Matrix_1.1-5     

loaded via a namespace (and not attached):
 [1] digest_0.6.8    evaluate_0.5.5  formatR_1.0     grid_3.1.3      htmltools_0.2.6
 [6] lattice_0.20-30 MASS_7.3-40     minqa_1.2.4     nlme_3.1-120    nloptr_1.0.4   
[11] rmarkdown_0.5.1 splines_3.1.3   stringr_0.6.2   tools_3.1.3     yaml_2.1.13    </code></pre>
<div class="references">
<h2>References</h2>
<p>Barr, Dale J, Roger Levy, Christoph Scheepers, and Harry J Tily. 2013. “Random Effects Structure for Confirmatory Hypothesis Testing: Keep It Maximal.” <em>Journal of Memory and Language</em> 68 (3). Elsevier: 255–78.</p>
<p>Kronm<span>ü</span>ller, Edmundo, and Dale J Barr. 2007. “Perspective-Free Pragmatics: Broken Precedents and the Recovery-from-Preemption Hypothesis.” <em>Journal of Memory and Language</em> 56 (3). Elsevier: 436–55.</p>
</div>
</div>



<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
